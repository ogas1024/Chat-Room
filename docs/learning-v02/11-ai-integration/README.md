# ç¬¬8ç« ï¼šAIæ™ºèƒ½åŠ©æ‰‹é›†æˆ

## ğŸ¯ å­¦ä¹ ç›®æ ‡

é€šè¿‡æœ¬ç« å­¦ä¹ ï¼Œæ‚¨å°†èƒ½å¤Ÿï¼š
- é›†æˆGLM-4-Flash AIæ¨¡å‹åˆ°Chat-Room
- è®¾è®¡æ™ºèƒ½å›å¤çš„è§¦å‘æœºåˆ¶
- å®ç°AIå¯¹è¯çš„ä¸Šä¸‹æ–‡ç®¡ç†
- ä¼˜åŒ–AIå“åº”çš„æ€§èƒ½å’Œè´¨é‡
- å¤„ç†AIæœåŠ¡çš„å¼‚å¸¸å’Œé™æµ
- ä¸ºChat-Roomæ·»åŠ æ™ºèƒ½åŒ–åŠŸèƒ½

## ğŸ“š ç« èŠ‚å†…å®¹

### 1. AIé›†æˆåŸºç¡€
- [GLM-4-Flash APIé›†æˆ](glm-api-integration.md)
- [AIå¯¹è¯ä¸Šä¸‹æ–‡ç®¡ç†](context-management.md)

### 2. æ™ºèƒ½åŠŸèƒ½å®ç°
- [æ™ºèƒ½å›å¤ç³»ç»Ÿ](smart-reply.md)
- [AIåŠŸèƒ½ä¼˜åŒ–ä¸ç›‘æ§](ai-optimization.md)

## ğŸ¤– AIé›†æˆæ¶æ„

```mermaid
graph TD
    A[Chat-Room AIç³»ç»Ÿ] --> B[AIè§¦å‘å™¨]
    A --> C[AIå¤„ç†å™¨]
    A --> D[ä¸Šä¸‹æ–‡ç®¡ç†å™¨]
    A --> E[å“åº”ç”Ÿæˆå™¨]
    
    B --> B1[@AI æåŠæ£€æµ‹]
    B --> B2[å…³é”®è¯è§¦å‘]
    B --> B3[æ™ºèƒ½å»ºè®®]
    
    C --> C1[GLM-4-Flash API]
    C --> C2[è¯·æ±‚ç®¡ç†å™¨]
    C --> C3[é”™è¯¯å¤„ç†å™¨]
    C --> C4[é™æµæ§åˆ¶å™¨]
    
    D --> D1[å¯¹è¯å†å²]
    D --> D2[ç”¨æˆ·åå¥½]
    D --> D3[ç¾¤ç»„ä¸Šä¸‹æ–‡]
    D --> D4[ä¼šè¯çŠ¶æ€]
    
    E --> E1[å›å¤æ ¼å¼åŒ–]
    E --> E2[å†…å®¹è¿‡æ»¤]
    E --> E3[å“åº”ä¼˜åŒ–]
    E --> E4[å¤šåª’ä½“æ”¯æŒ]
    
    style A fill:#e8f5e8
    style B fill:#fff3cd
    style C fill:#f8d7da
    style D fill:#e1f5fe
    style E fill:#f3e5f5
```

## ğŸ”„ AIå¯¹è¯æµç¨‹

```mermaid
sequenceDiagram
    participant U as ç”¨æˆ·
    participant C as Chat-Roomå®¢æˆ·ç«¯
    participant S as Chat-RoomæœåŠ¡å™¨
    participant AI as AIå¤„ç†å™¨
    participant GLM as GLM-4-Flash API
    
    Note over U,GLM: AIæ™ºèƒ½å›å¤æµç¨‹
    
    U->>C: 1. å‘é€æ¶ˆæ¯ "@AI ä½ å¥½"
    C->>S: 2. è½¬å‘æ¶ˆæ¯åˆ°æœåŠ¡å™¨
    S->>AI: 3. æ£€æµ‹AIè§¦å‘æ¡ä»¶
    
    alt AIè§¦å‘æˆåŠŸ
        AI->>AI: 4. æå–ç”¨æˆ·é—®é¢˜
        AI->>AI: 5. æ„å»ºå¯¹è¯ä¸Šä¸‹æ–‡
        AI->>GLM: 6. è°ƒç”¨GLM-4-Flash API
        GLM-->>AI: 7. è¿”å›AIå›å¤
        AI->>AI: 8. å¤„ç†å’Œä¼˜åŒ–å›å¤
        AI->>S: 9. å‘é€AIå›å¤
        S->>C: 10. å¹¿æ’­AIå›å¤
        C->>U: 11. æ˜¾ç¤ºAIå›å¤
    else éAIæ¶ˆæ¯
        S->>C: æ­£å¸¸æ¶ˆæ¯å¤„ç†
    end
    
    Note over AI: æ›´æ–°å¯¹è¯ä¸Šä¸‹æ–‡
    AI->>AI: 12. ä¿å­˜å¯¹è¯å†å²
```

## ğŸ§  AIé›†æˆæ ¸å¿ƒå®ç°

### GLM-4-Flash APIé›†æˆ

```python
"""
Chat-Room AIæ™ºèƒ½åŠ©æ‰‹é›†æˆ
åŸºäºGLM-4-Flash APIå®ç°æ™ºèƒ½å›å¤åŠŸèƒ½
"""

import asyncio
import json
import time
import re
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, asdict
import aiohttp
import threading
from collections import deque
from shared.logger import get_logger

logger = get_logger("ai.integration")

@dataclass
class AIConfig:
    """AIé…ç½®"""
    api_key: str
    api_base_url: str = "https://open.bigmodel.cn/api/paas/v4/"
    model_name: str = "glm-4-flash"
    max_tokens: int = 1000
    temperature: float = 0.7
    timeout: float = 30.0
    max_retries: int = 3
    rate_limit_per_minute: int = 60

@dataclass
class ConversationMessage:
    """å¯¹è¯æ¶ˆæ¯"""
    role: str  # "user", "assistant", "system"
    content: str
    timestamp: float = None
    
    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = time.time()

@dataclass
class AIResponse:
    """AIå“åº”"""
    content: str
    model: str
    usage: Dict[str, int]
    finish_reason: str
    response_time: float

class ConversationContext:
    """å¯¹è¯ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
    
    def __init__(self, max_history: int = 10):
        self.max_history = max_history
        self.messages: deque = deque(maxlen=max_history)
        self.user_preferences: Dict[str, Any] = {}
        self.group_context: Dict[str, Any] = {}
    
    def add_message(self, role: str, content: str):
        """æ·»åŠ æ¶ˆæ¯åˆ°ä¸Šä¸‹æ–‡"""
        message = ConversationMessage(role=role, content=content)
        self.messages.append(message)
        logger.debug(f"æ·»åŠ æ¶ˆæ¯åˆ°ä¸Šä¸‹æ–‡: {role} - {content[:50]}...")
    
    def get_context_messages(self) -> List[Dict[str, str]]:
        """è·å–ä¸Šä¸‹æ–‡æ¶ˆæ¯åˆ—è¡¨"""
        return [{"role": msg.role, "content": msg.content} for msg in self.messages]
    
    def clear_context(self):
        """æ¸…ç©ºä¸Šä¸‹æ–‡"""
        self.messages.clear()
        logger.info("æ¸…ç©ºå¯¹è¯ä¸Šä¸‹æ–‡")
    
    def set_system_prompt(self, prompt: str):
        """è®¾ç½®ç³»ç»Ÿæç¤º"""
        # ç§»é™¤æ—§çš„ç³»ç»Ÿæ¶ˆæ¯
        self.messages = deque([msg for msg in self.messages if msg.role != "system"], 
                             maxlen=self.max_history)
        
        # æ·»åŠ æ–°çš„ç³»ç»Ÿæ¶ˆæ¯åˆ°å¼€å¤´
        system_msg = ConversationMessage(role="system", content=prompt)
        self.messages.appendleft(system_msg)
        
        logger.info("è®¾ç½®ç³»ç»Ÿæç¤º")

class RateLimiter:
    """APIé™æµå™¨"""
    
    def __init__(self, max_requests_per_minute: int):
        self.max_requests = max_requests_per_minute
        self.requests = deque()
        self.lock = threading.Lock()
    
    def can_make_request(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å¯ä»¥å‘èµ·è¯·æ±‚"""
        current_time = time.time()
        
        with self.lock:
            # ç§»é™¤ä¸€åˆ†é’Ÿå‰çš„è¯·æ±‚è®°å½•
            while self.requests and current_time - self.requests[0] > 60:
                self.requests.popleft()
            
            # æ£€æŸ¥æ˜¯å¦è¶…è¿‡é™åˆ¶
            if len(self.requests) >= self.max_requests:
                return False
            
            # è®°å½•å½“å‰è¯·æ±‚
            self.requests.append(current_time)
            return True
    
    def get_wait_time(self) -> float:
        """è·å–éœ€è¦ç­‰å¾…çš„æ—¶é—´"""
        if not self.requests:
            return 0.0
        
        current_time = time.time()
        oldest_request = self.requests[0]
        
        if len(self.requests) >= self.max_requests:
            return 60 - (current_time - oldest_request)
        
        return 0.0

class GLMAPIClient:
    """GLM-4-Flash APIå®¢æˆ·ç«¯"""
    
    def __init__(self, config: AIConfig):
        self.config = config
        self.rate_limiter = RateLimiter(config.rate_limit_per_minute)
        self.session: Optional[aiohttp.ClientSession] = None
    
    async def _ensure_session(self):
        """ç¡®ä¿HTTPä¼šè¯å­˜åœ¨"""
        if self.session is None or self.session.closed:
            timeout = aiohttp.ClientTimeout(total=self.config.timeout)
            self.session = aiohttp.ClientSession(timeout=timeout)
    
    async def _make_request(self, messages: List[Dict[str, str]]) -> Dict[str, Any]:
        """å‘èµ·APIè¯·æ±‚"""
        
        await self._ensure_session()
        
        headers = {
            "Authorization": f"Bearer {self.config.api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": self.config.model_name,
            "messages": messages,
            "max_tokens": self.config.max_tokens,
            "temperature": self.config.temperature,
            "stream": False
        }
        
        url = f"{self.config.api_base_url}chat/completions"
        
        async with self.session.post(url, headers=headers, json=payload) as response:
            if response.status == 200:
                return await response.json()
            else:
                error_text = await response.text()
                raise Exception(f"APIè¯·æ±‚å¤±è´¥: {response.status} - {error_text}")
    
    async def generate_response(self, messages: List[Dict[str, str]]) -> AIResponse:
        """ç”ŸæˆAIå›å¤"""
        
        # æ£€æŸ¥é™æµ
        if not self.rate_limiter.can_make_request():
            wait_time = self.rate_limiter.get_wait_time()
            raise Exception(f"APIé™æµï¼Œè¯·ç­‰å¾… {wait_time:.1f} ç§’")
        
        start_time = time.time()
        
        for attempt in range(self.config.max_retries):
            try:
                logger.debug(f"å‘èµ·APIè¯·æ±‚ (å°è¯• {attempt + 1}/{self.config.max_retries})")
                
                response_data = await self._make_request(messages)
                
                # è§£æå“åº”
                choice = response_data["choices"][0]
                content = choice["message"]["content"]
                finish_reason = choice["finish_reason"]
                usage = response_data.get("usage", {})
                
                response_time = time.time() - start_time
                
                ai_response = AIResponse(
                    content=content,
                    model=self.config.model_name,
                    usage=usage,
                    finish_reason=finish_reason,
                    response_time=response_time
                )
                
                logger.info(f"AIå›å¤ç”ŸæˆæˆåŠŸ (è€—æ—¶: {response_time:.2f}s)")
                return ai_response
                
            except Exception as e:
                logger.warning(f"APIè¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}): {e}")
                
                if attempt == self.config.max_retries - 1:
                    raise Exception(f"APIè¯·æ±‚å¤±è´¥ï¼Œå·²é‡è¯• {self.config.max_retries} æ¬¡: {e}")
                
                # æŒ‡æ•°é€€é¿
                await asyncio.sleep(2 ** attempt)
    
    async def close(self):
        """å…³é—­HTTPä¼šè¯"""
        if self.session and not self.session.closed:
            await self.session.close()

class ChatRoomAI:
    """Chat-Room AIåŠ©æ‰‹"""
    
    def __init__(self, config: AIConfig):
        self.config = config
        self.api_client = GLMAPIClient(config)
        self.contexts: Dict[str, ConversationContext] = {}  # æŒ‰ç¾¤ç»„æˆ–ç”¨æˆ·IDç®¡ç†ä¸Šä¸‹æ–‡
        self.ai_trigger_patterns = [
            r"@AI\s+(.+)",  # @AI åè·Ÿé—®é¢˜
            r"@ai\s+(.+)",  # å°å†™ai
            r"@æ™ºèƒ½åŠ©æ‰‹\s+(.+)",  # ä¸­æ–‡åˆ«å
        ]
        
        # ç³»ç»Ÿæç¤ºæ¨¡æ¿
        self.system_prompt = """ä½ æ˜¯Chat-RoomèŠå¤©å®¤çš„æ™ºèƒ½åŠ©æ‰‹ï¼Œåå­—å«å°æ™ºã€‚ä½ çš„ç‰¹ç‚¹ï¼š
1. å‹å¥½ã€æœ‰å¸®åŠ©ã€æœ‰è¶£
2. å›å¤ç®€æ´æ˜äº†ï¼Œé€šå¸¸ä¸è¶…è¿‡200å­—
3. å¯ä»¥å›ç­”å„ç§é—®é¢˜ï¼Œæä¾›å»ºè®®å’Œå¸®åŠ©
4. äº†è§£èŠå¤©å®¤çš„åŠŸèƒ½å’Œä½¿ç”¨æ–¹æ³•
5. ä¿æŒç§¯ææ­£é¢çš„æ€åº¦

è¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ç»™å‡ºåˆé€‚çš„å›å¤ã€‚"""
    
    def get_context(self, context_id: str) -> ConversationContext:
        """è·å–å¯¹è¯ä¸Šä¸‹æ–‡"""
        if context_id not in self.contexts:
            context = ConversationContext()
            context.set_system_prompt(self.system_prompt)
            self.contexts[context_id] = context
        
        return self.contexts[context_id]
    
    def is_ai_trigger(self, message: str) -> Tuple[bool, str]:
        """æ£€æŸ¥æ˜¯å¦è§¦å‘AIå›å¤"""
        
        for pattern in self.ai_trigger_patterns:
            match = re.search(pattern, message, re.IGNORECASE)
            if match:
                user_question = match.group(1).strip()
                return True, user_question
        
        return False, ""
    
    async def process_message(self, user_id: int, username: str, message: str, 
                            group_id: Optional[int] = None) -> Optional[str]:
        """å¤„ç†ç”¨æˆ·æ¶ˆæ¯ï¼Œç”ŸæˆAIå›å¤"""
        
        # æ£€æŸ¥æ˜¯å¦è§¦å‘AI
        is_trigger, user_question = self.is_ai_trigger(message)
        if not is_trigger:
            return None
        
        try:
            # ç¡®å®šä¸Šä¸‹æ–‡ID
            context_id = f"group_{group_id}" if group_id else f"user_{user_id}"
            context = self.get_context(context_id)
            
            # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°ä¸Šä¸‹æ–‡
            context.add_message("user", user_question)
            
            # è·å–ä¸Šä¸‹æ–‡æ¶ˆæ¯
            messages = context.get_context_messages()
            
            # ç”ŸæˆAIå›å¤
            ai_response = await self.api_client.generate_response(messages)
            
            # æ·»åŠ AIå›å¤åˆ°ä¸Šä¸‹æ–‡
            context.add_message("assistant", ai_response.content)
            
            # æ ¼å¼åŒ–å›å¤
            formatted_response = self._format_ai_response(ai_response.content, username)
            
            logger.info(f"AIå›å¤ç”Ÿæˆ: {username} -> {user_question[:30]}... -> {ai_response.content[:50]}...")
            
            return formatted_response
            
        except Exception as e:
            logger.error(f"AIå¤„ç†å¤±è´¥: {e}")
            return f"@{username} æŠ±æ­‰ï¼Œæˆ‘ç°åœ¨æœ‰ç‚¹å¿™ï¼Œè¯·ç¨åå†è¯•ã€‚"
    
    def _format_ai_response(self, content: str, username: str) -> str:
        """æ ¼å¼åŒ–AIå›å¤"""
        
        # æ·»åŠ ç”¨æˆ·æåŠ
        if not content.startswith(f"@{username}"):
            content = f"@{username} {content}"
        
        # æ·»åŠ AIæ ‡è¯†
        if not content.endswith("ğŸ¤–"):
            content = f"{content} ğŸ¤–"
        
        return content
    
    def clear_context(self, context_id: str):
        """æ¸…ç©ºæŒ‡å®šä¸Šä¸‹æ–‡"""
        if context_id in self.contexts:
            self.contexts[context_id].clear_context()
    
    def get_ai_stats(self) -> Dict[str, Any]:
        """è·å–AIä½¿ç”¨ç»Ÿè®¡"""
        
        total_contexts = len(self.contexts)
        total_messages = sum(len(ctx.messages) for ctx in self.contexts.values())
        
        return {
            "total_contexts": total_contexts,
            "total_messages": total_messages,
            "active_contexts": [ctx_id for ctx_id, ctx in self.contexts.items() 
                              if len(ctx.messages) > 1]  # æœ‰å¯¹è¯çš„ä¸Šä¸‹æ–‡
        }
    
    async def close(self):
        """å…³é—­AIå®¢æˆ·ç«¯"""
        await self.api_client.close()

# AIæ¶ˆæ¯å¤„ç†å™¨é›†æˆåˆ°Chat-RoomæœåŠ¡å™¨
class AIMessageHandler:
    """AIæ¶ˆæ¯å¤„ç†å™¨"""
    
    def __init__(self, ai_config: AIConfig):
        self.ai = ChatRoomAI(ai_config)
        self.processing_queue = asyncio.Queue()
        self.is_running = False
        
    async def start(self):
        """å¯åŠ¨AIå¤„ç†å™¨"""
        self.is_running = True
        
        # å¯åŠ¨æ¶ˆæ¯å¤„ç†ä»»åŠ¡
        asyncio.create_task(self._process_messages())
        logger.info("AIæ¶ˆæ¯å¤„ç†å™¨å¯åŠ¨")
    
    async def stop(self):
        """åœæ­¢AIå¤„ç†å™¨"""
        self.is_running = False
        await self.ai.close()
        logger.info("AIæ¶ˆæ¯å¤„ç†å™¨åœæ­¢")
    
    async def handle_message(self, user_id: int, username: str, message: str, 
                           group_id: Optional[int] = None) -> Optional[str]:
        """å¤„ç†æ¶ˆæ¯ï¼ˆå¼‚æ­¥ï¼‰"""
        
        try:
            # å°†æ¶ˆæ¯åŠ å…¥å¤„ç†é˜Ÿåˆ—
            await self.processing_queue.put({
                "user_id": user_id,
                "username": username,
                "message": message,
                "group_id": group_id
            })
            
            # å¤„ç†æ¶ˆæ¯
            ai_response = await self.ai.process_message(user_id, username, message, group_id)
            return ai_response
            
        except Exception as e:
            logger.error(f"AIæ¶ˆæ¯å¤„ç†å¤±è´¥: {e}")
            return None
    
    async def _process_messages(self):
        """åå°æ¶ˆæ¯å¤„ç†ä»»åŠ¡"""
        
        while self.is_running:
            try:
                # ä»é˜Ÿåˆ—è·å–æ¶ˆæ¯
                message_data = await asyncio.wait_for(
                    self.processing_queue.get(), timeout=1.0
                )
                
                # å¤„ç†æ¶ˆæ¯
                await self.ai.process_message(
                    message_data["user_id"],
                    message_data["username"],
                    message_data["message"],
                    message_data["group_id"]
                )
                
            except asyncio.TimeoutError:
                continue
            except Exception as e:
                logger.error(f"åå°æ¶ˆæ¯å¤„ç†å¤±è´¥: {e}")

# ä½¿ç”¨ç¤ºä¾‹
async def demonstrate_ai_integration():
    """æ¼”ç¤ºAIé›†æˆåŠŸèƒ½"""
    
    print("=== Chat-Room AIé›†æˆæ¼”ç¤º ===")
    
    # é…ç½®AIï¼ˆéœ€è¦çœŸå®çš„APIå¯†é’¥ï¼‰
    ai_config = AIConfig(
        api_key="your-api-key-here",  # æ›¿æ¢ä¸ºçœŸå®çš„APIå¯†é’¥
        max_tokens=500,
        temperature=0.7
    )
    
    # åˆ›å»ºAIåŠ©æ‰‹
    ai = ChatRoomAI(ai_config)
    
    try:
        # æ¨¡æ‹Ÿç”¨æˆ·æ¶ˆæ¯
        test_messages = [
            "@AI ä½ å¥½ï¼Œä½ æ˜¯è°ï¼Ÿ",
            "@ai ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ",
            "@æ™ºèƒ½åŠ©æ‰‹ èƒ½å¸®æˆ‘ä»‹ç»ä¸€ä¸‹è¿™ä¸ªèŠå¤©å®¤å—ï¼Ÿ",
            "æ™®é€šæ¶ˆæ¯ï¼Œä¸è§¦å‘AI",
            "@AI è°¢è°¢ä½ çš„å¸®åŠ©ï¼"
        ]
        
        for message in test_messages:
            print(f"\nç”¨æˆ·æ¶ˆæ¯: {message}")
            
            ai_response = await ai.process_message(
                user_id=1,
                username="æµ‹è¯•ç”¨æˆ·",
                message=message,
                group_id=1
            )
            
            if ai_response:
                print(f"AIå›å¤: {ai_response}")
            else:
                print("æœªè§¦å‘AIå›å¤")
        
        # æ˜¾ç¤ºAIç»Ÿè®¡
        stats = ai.get_ai_stats()
        print(f"\nAIä½¿ç”¨ç»Ÿè®¡: {stats}")
        
    finally:
        await ai.close()

if __name__ == "__main__":
    # æ³¨æ„ï¼šéœ€è¦çœŸå®çš„GLM-4-Flash APIå¯†é’¥æ‰èƒ½è¿è¡Œ
    # asyncio.run(demonstrate_ai_integration())
    print("AIé›†æˆæ¼”ç¤ºä»£ç å·²å‡†å¤‡å°±ç»ªï¼Œéœ€è¦é…ç½®çœŸå®çš„APIå¯†é’¥")
```

## ğŸ“‹ å­¦ä¹ æ£€æŸ¥æ¸…å•

å®Œæˆæœ¬ç« å­¦ä¹ åï¼Œè¯·ç¡®è®¤æ‚¨èƒ½å¤Ÿï¼š

### AI APIé›†æˆ
- [ ] é›†æˆGLM-4-Flash APIåˆ°Chat-Room
- [ ] å¤„ç†APIè®¤è¯å’Œè¯·æ±‚
- [ ] å®ç°APIé”™è¯¯å¤„ç†å’Œé‡è¯•
- [ ] ç®¡ç†APIé™æµå’Œé…é¢

### æ™ºèƒ½å›å¤ç³»ç»Ÿ
- [ ] è®¾è®¡AIè§¦å‘æœºåˆ¶ï¼ˆ@AIæåŠï¼‰
- [ ] å®ç°å¯¹è¯ä¸Šä¸‹æ–‡ç®¡ç†
- [ ] æ ¼å¼åŒ–AIå›å¤å†…å®¹
- [ ] å¤„ç†å¤šç”¨æˆ·å¹¶å‘AIè¯·æ±‚

### æ€§èƒ½ä¼˜åŒ–
- [ ] å®ç°å¼‚æ­¥AIå¤„ç†
- [ ] ä¼˜åŒ–APIè°ƒç”¨æ€§èƒ½
- [ ] ç®¡ç†å†…å­˜ä¸­çš„å¯¹è¯ä¸Šä¸‹æ–‡
- [ ] å®ç°AIåŠŸèƒ½çš„ç›‘æ§

### ç”¨æˆ·ä½“éªŒ
- [ ] è®¾è®¡å‹å¥½çš„AIäº¤äº’æ–¹å¼
- [ ] æä¾›æ¸…æ™°çš„AIå›å¤æ ‡è¯†
- [ ] å®ç°AIåŠŸèƒ½çš„å¼€å…³æ§åˆ¶
- [ ] å¤„ç†AIæœåŠ¡ä¸å¯ç”¨çš„æƒ…å†µ

## ğŸ”— ç›¸å…³èµ„æº

- [GLM-4-Flash APIæ–‡æ¡£](https://open.bigmodel.cn/dev/api)
- [OpenAI APIå‚è€ƒ](https://platform.openai.com/docs/api-reference)
- [å¼‚æ­¥ç¼–ç¨‹æœ€ä½³å®è·µ](https://docs.python.org/3/library/asyncio.html)
- [AIèŠå¤©æœºå™¨äººè®¾è®¡æŒ‡å—](https://chatbotsmagazine.com/chatbot-design-best-practices-2019-4c8c2b8b5b5a)

## ğŸ“š ä¸‹ä¸€æ­¥

AIæ™ºèƒ½åŠ©æ‰‹é›†æˆå®Œæˆåï¼Œè¯·ç»§ç»­å­¦ä¹ ï¼š
- [GLM-4-Flash APIé›†æˆ](glm-api-integration.md)

---

**ä¸ºChat-Roomæ³¨å…¥AIæ™ºèƒ½ï¼Œåˆ›é€ æ›´æ™ºèƒ½çš„èŠå¤©ä½“éªŒï¼** ğŸ¤–
