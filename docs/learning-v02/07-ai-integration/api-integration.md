# APIé›†æˆåŸºç¡€

## ğŸ¯ å­¦ä¹ ç›®æ ‡

é€šè¿‡æœ¬ç« å­¦ä¹ ï¼Œæ‚¨å°†èƒ½å¤Ÿï¼š
- ç†è§£AI APIé›†æˆçš„åŸºæœ¬åŸç†å’Œæœ€ä½³å®è·µ
- æŒæ¡GLM-4-Flash APIçš„ä½¿ç”¨æ–¹æ³•å’Œç‰¹æ€§
- å­¦ä¼šè®¾è®¡å¯é çš„APIè°ƒç”¨å’Œé”™è¯¯å¤„ç†æœºåˆ¶
- åœ¨Chat-Roomé¡¹ç›®ä¸­å®ç°æ™ºèƒ½AIåŠ©æ‰‹åŠŸèƒ½

## ğŸ¤– AIé›†æˆæ¶æ„

### AIé›†æˆç³»ç»Ÿæ¦‚è§ˆ

```mermaid
graph TB
    subgraph "AIé›†æˆæ¶æ„"
        A[ç”¨æˆ·æ¶ˆæ¯<br/>User Message] --> B[æ¶ˆæ¯é¢„å¤„ç†<br/>Message Preprocessing]
        B --> C[AIè§¦å‘æ£€æµ‹<br/>AI Trigger Detection]
        C --> D[ä¸Šä¸‹æ–‡ç®¡ç†<br/>Context Management]
        D --> E[APIè°ƒç”¨ç®¡ç†<br/>API Call Manager]
        
        E --> F[GLM-4-Flash API<br/>GLM-4-Flash API]
        F --> G[å“åº”å¤„ç†<br/>Response Processing]
        G --> H[åå¤„ç†<br/>Post Processing]
        H --> I[æ¶ˆæ¯å‘é€<br/>Message Sending]
        
        J[é…ç½®ç®¡ç†<br/>Config Manager] --> E
        K[ç¼“å­˜ç³»ç»Ÿ<br/>Cache System] --> E
        L[é™æµæ§åˆ¶<br/>Rate Limiting] --> E
        M[é”™è¯¯å¤„ç†<br/>Error Handling] --> E
    end
    
    subgraph "AIåŠŸèƒ½æ¨¡å—"
        N[æ™ºèƒ½å›å¤<br/>Smart Reply]
        O[å†…å®¹æ€»ç»“<br/>Content Summary]
        P[è¯­è¨€ç¿»è¯‘<br/>Translation]
        Q[æƒ…æ„Ÿåˆ†æ<br/>Sentiment Analysis]
    end
    
    G --> N
    G --> O
    G --> P
    G --> Q
    
    style A fill:#e8f5e8
    style F fill:#f8d7da
    style I fill:#fff3cd
```

### APIè°ƒç”¨æµç¨‹

```mermaid
sequenceDiagram
    participant U as ç”¨æˆ·
    participant C as Chat-Roomå®¢æˆ·ç«¯
    participant S as Chat-RoomæœåŠ¡å™¨
    participant AI as AIç®¡ç†å™¨
    participant GLM as GLM-4-Flash API
    participant DB as æ•°æ®åº“
    
    Note over U,DB: AIåŠ©æ‰‹è°ƒç”¨æµç¨‹
    
    U->>C: å‘é€æ¶ˆæ¯ "@AI ä½ å¥½"
    C->>S: è½¬å‘æ¶ˆæ¯
    S->>AI: æ£€æµ‹AIè§¦å‘
    AI->>AI: è§£æç”¨æˆ·æ„å›¾
    AI->>DB: è·å–ä¸Šä¸‹æ–‡å†å²
    DB->>AI: è¿”å›å†å²å¯¹è¯
    
    AI->>AI: æ„å»ºAPIè¯·æ±‚
    AI->>GLM: è°ƒç”¨GLM-4-Flash API
    
    alt APIè°ƒç”¨æˆåŠŸ
        GLM->>AI: è¿”å›AIå“åº”
        AI->>AI: å¤„ç†å“åº”å†…å®¹
        AI->>S: è¿”å›å¤„ç†ç»“æœ
        S->>C: å¹¿æ’­AIå›å¤
        C->>U: æ˜¾ç¤ºAIå›å¤
        AI->>DB: ä¿å­˜å¯¹è¯è®°å½•
    else APIè°ƒç”¨å¤±è´¥
        GLM->>AI: è¿”å›é”™è¯¯ä¿¡æ¯
        AI->>AI: é”™è¯¯å¤„ç†å’Œé‡è¯•
        AI->>S: è¿”å›é”™è¯¯æç¤º
        S->>C: å‘é€é”™è¯¯æ¶ˆæ¯
    end
```

## ğŸ”Œ APIé›†æˆå®ç°

### GLM-4-Flash APIå®¢æˆ·ç«¯

```python
# server/ai/glm_client.py - GLM-4-Flash APIå®¢æˆ·ç«¯
import asyncio
import aiohttp
import json
import time
import hashlib
from typing import Dict, List, Optional, Any, AsyncGenerator
from dataclasses import dataclass, asdict
from enum import Enum
import logging

class MessageRole(Enum):
    """æ¶ˆæ¯è§’è‰²æšä¸¾"""
    SYSTEM = "system"
    USER = "user"
    ASSISTANT = "assistant"

@dataclass
class ChatMessage:
    """èŠå¤©æ¶ˆæ¯"""
    role: MessageRole
    content: str
    timestamp: Optional[float] = None
    
    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = time.time()
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºAPIæ ¼å¼"""
        return {
            "role": self.role.value,
            "content": self.content
        }

@dataclass
class GLMConfig:
    """GLMé…ç½®"""
    api_key: str
    base_url: str = "https://open.bigmodel.cn/api/paas/v4"
    model: str = "glm-4-flash"
    max_tokens: int = 1024
    temperature: float = 0.7
    top_p: float = 0.9
    timeout: int = 30
    max_retries: int = 3
    retry_delay: float = 1.0

@dataclass
class APIResponse:
    """APIå“åº”"""
    success: bool
    content: Optional[str] = None
    error_message: Optional[str] = None
    error_code: Optional[str] = None
    usage: Optional[Dict[str, int]] = None
    response_time: Optional[float] = None

class GLMClient:
    """
    GLM-4-Flash APIå®¢æˆ·ç«¯
    
    åŠŸèƒ½ï¼š
    1. å¼‚æ­¥APIè°ƒç”¨
    2. è‡ªåŠ¨é‡è¯•æœºåˆ¶
    3. é™æµæ§åˆ¶
    4. é”™è¯¯å¤„ç†
    5. å“åº”ç¼“å­˜
    """
    
    def __init__(self, config: GLMConfig):
        self.config = config
        self.logger = logging.getLogger('GLMClient')
        
        # é™æµæ§åˆ¶
        self.rate_limiter = asyncio.Semaphore(5)  # æœ€å¤š5ä¸ªå¹¶å‘è¯·æ±‚
        self.last_request_time = 0
        self.min_request_interval = 0.1  # æœ€å°è¯·æ±‚é—´éš”100ms
        
        # ä¼šè¯ç®¡ç†
        self.session: Optional[aiohttp.ClientSession] = None
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'total_tokens_used': 0,
            'total_response_time': 0.0
        }
    
    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        await self.start()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        await self.close()
    
    async def start(self):
        """å¯åŠ¨å®¢æˆ·ç«¯"""
        if self.session is None:
            timeout = aiohttp.ClientTimeout(total=self.config.timeout)
            self.session = aiohttp.ClientSession(timeout=timeout)
    
    async def close(self):
        """å…³é—­å®¢æˆ·ç«¯"""
        if self.session:
            await self.session.close()
            self.session = None
    
    async def chat_completion(self, messages: List[ChatMessage], 
                            stream: bool = False) -> APIResponse:
        """
        èŠå¤©å®ŒæˆAPIè°ƒç”¨
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            stream: æ˜¯å¦ä½¿ç”¨æµå¼å“åº”
            
        Returns:
            APIå“åº”ç»“æœ
        """
        if not self.session:
            await self.start()
        
        # é™æµæ§åˆ¶
        async with self.rate_limiter:
            await self._wait_for_rate_limit()
            
            # æ„å»ºè¯·æ±‚
            request_data = self._build_request(messages, stream)
            
            # æ‰§è¡Œè¯·æ±‚
            start_time = time.time()
            response = await self._make_request(request_data, stream)
            response.response_time = time.time() - start_time
            
            # æ›´æ–°ç»Ÿè®¡
            self._update_stats(response)
            
            return response
    
    async def simple_chat(self, user_message: str, 
                         system_prompt: str = None) -> APIResponse:
        """
        ç®€å•èŠå¤©æ¥å£
        
        Args:
            user_message: ç”¨æˆ·æ¶ˆæ¯
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            
        Returns:
            APIå“åº”ç»“æœ
        """
        messages = []
        
        # æ·»åŠ ç³»ç»Ÿæç¤ºè¯
        if system_prompt:
            messages.append(ChatMessage(MessageRole.SYSTEM, system_prompt))
        
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
        messages.append(ChatMessage(MessageRole.USER, user_message))
        
        return await self.chat_completion(messages)
    
    async def chat_with_context(self, user_message: str, 
                               conversation_history: List[ChatMessage],
                               system_prompt: str = None) -> APIResponse:
        """
        å¸¦ä¸Šä¸‹æ–‡çš„èŠå¤©
        
        Args:
            user_message: ç”¨æˆ·æ¶ˆæ¯
            conversation_history: å¯¹è¯å†å²
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            
        Returns:
            APIå“åº”ç»“æœ
        """
        messages = []
        
        # æ·»åŠ ç³»ç»Ÿæç¤ºè¯
        if system_prompt:
            messages.append(ChatMessage(MessageRole.SYSTEM, system_prompt))
        
        # æ·»åŠ å†å²å¯¹è¯ï¼ˆé™åˆ¶é•¿åº¦ï¼‰
        max_history = 10  # æœ€å¤šä¿ç•™10è½®å¯¹è¯
        recent_history = conversation_history[-max_history:] if len(conversation_history) > max_history else conversation_history
        messages.extend(recent_history)
        
        # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
        messages.append(ChatMessage(MessageRole.USER, user_message))
        
        return await self.chat_completion(messages)
    
    def _build_request(self, messages: List[ChatMessage], stream: bool) -> Dict[str, Any]:
        """æ„å»ºAPIè¯·æ±‚"""
        return {
            "model": self.config.model,
            "messages": [msg.to_dict() for msg in messages],
            "max_tokens": self.config.max_tokens,
            "temperature": self.config.temperature,
            "top_p": self.config.top_p,
            "stream": stream
        }
    
    async def _make_request(self, request_data: Dict[str, Any], 
                          stream: bool) -> APIResponse:
        """æ‰§è¡ŒAPIè¯·æ±‚"""
        headers = {
            "Authorization": f"Bearer {self.config.api_key}",
            "Content-Type": "application/json"
        }
        
        url = f"{self.config.base_url}/chat/completions"
        
        for attempt in range(self.config.max_retries):
            try:
                async with self.session.post(url, json=request_data, headers=headers) as response:
                    if response.status == 200:
                        if stream:
                            return await self._handle_stream_response(response)
                        else:
                            return await self._handle_normal_response(response)
                    else:
                        error_text = await response.text()
                        self.logger.warning(f"APIè¯·æ±‚å¤±è´¥ (çŠ¶æ€ç : {response.status}): {error_text}")
                        
                        if attempt < self.config.max_retries - 1:
                            await asyncio.sleep(self.config.retry_delay * (2 ** attempt))
                            continue
                        
                        return APIResponse(
                            success=False,
                            error_message=f"HTTP {response.status}: {error_text}",
                            error_code=str(response.status)
                        )
            
            except asyncio.TimeoutError:
                self.logger.warning(f"APIè¯·æ±‚è¶…æ—¶ (å°è¯• {attempt + 1}/{self.config.max_retries})")
                if attempt < self.config.max_retries - 1:
                    await asyncio.sleep(self.config.retry_delay * (2 ** attempt))
                    continue
                
                return APIResponse(
                    success=False,
                    error_message="è¯·æ±‚è¶…æ—¶",
                    error_code="TIMEOUT"
                )
            
            except Exception as e:
                self.logger.error(f"APIè¯·æ±‚å¼‚å¸¸ (å°è¯• {attempt + 1}/{self.config.max_retries}): {e}")
                if attempt < self.config.max_retries - 1:
                    await asyncio.sleep(self.config.retry_delay * (2 ** attempt))
                    continue
                
                return APIResponse(
                    success=False,
                    error_message=str(e),
                    error_code="REQUEST_ERROR"
                )
        
        return APIResponse(
            success=False,
            error_message="æ‰€æœ‰é‡è¯•å°è¯•éƒ½å¤±è´¥äº†",
            error_code="MAX_RETRIES_EXCEEDED"
        )
    
    async def _handle_normal_response(self, response: aiohttp.ClientResponse) -> APIResponse:
        """å¤„ç†æ™®é€šå“åº”"""
        try:
            data = await response.json()
            
            if "choices" in data and len(data["choices"]) > 0:
                content = data["choices"][0]["message"]["content"]
                usage = data.get("usage", {})
                
                return APIResponse(
                    success=True,
                    content=content,
                    usage=usage
                )
            else:
                return APIResponse(
                    success=False,
                    error_message="å“åº”æ ¼å¼é”™è¯¯ï¼šç¼ºå°‘choiceså­—æ®µ",
                    error_code="INVALID_RESPONSE"
                )
        
        except json.JSONDecodeError as e:
            return APIResponse(
                success=False,
                error_message=f"JSONè§£æé”™è¯¯: {e}",
                error_code="JSON_DECODE_ERROR"
            )
    
    async def _handle_stream_response(self, response: aiohttp.ClientResponse) -> APIResponse:
        """å¤„ç†æµå¼å“åº”"""
        content_parts = []
        
        try:
            async for line in response.content:
                line = line.decode('utf-8').strip()
                
                if line.startswith('data: '):
                    data_str = line[6:]  # ç§»é™¤ 'data: ' å‰ç¼€
                    
                    if data_str == '[DONE]':
                        break
                    
                    try:
                        data = json.loads(data_str)
                        if "choices" in data and len(data["choices"]) > 0:
                            delta = data["choices"][0].get("delta", {})
                            if "content" in delta:
                                content_parts.append(delta["content"])
                    except json.JSONDecodeError:
                        continue
            
            full_content = ''.join(content_parts)
            
            return APIResponse(
                success=True,
                content=full_content
            )
        
        except Exception as e:
            return APIResponse(
                success=False,
                error_message=f"æµå¼å“åº”å¤„ç†é”™è¯¯: {e}",
                error_code="STREAM_ERROR"
            )
    
    async def _wait_for_rate_limit(self):
        """ç­‰å¾…é™æµ"""
        current_time = time.time()
        time_since_last = current_time - self.last_request_time
        
        if time_since_last < self.min_request_interval:
            await asyncio.sleep(self.min_request_interval - time_since_last)
        
        self.last_request_time = time.time()
    
    def _update_stats(self, response: APIResponse):
        """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
        self.stats['total_requests'] += 1
        
        if response.success:
            self.stats['successful_requests'] += 1
            if response.usage:
                self.stats['total_tokens_used'] += response.usage.get('total_tokens', 0)
        else:
            self.stats['failed_requests'] += 1
        
        if response.response_time:
            self.stats['total_response_time'] += response.response_time
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.stats.copy()
        
        if stats['total_requests'] > 0:
            stats['success_rate'] = stats['successful_requests'] / stats['total_requests']
            stats['average_response_time'] = stats['total_response_time'] / stats['total_requests']
        else:
            stats['success_rate'] = 0.0
            stats['average_response_time'] = 0.0
        
        return stats

# ä½¿ç”¨ç¤ºä¾‹
async def demo_glm_client():
    """GLMå®¢æˆ·ç«¯æ¼”ç¤º"""
    # é…ç½®ï¼ˆå®é™…ä½¿ç”¨æ—¶éœ€è¦çœŸå®çš„APIå¯†é’¥ï¼‰
    config = GLMConfig(
        api_key="your_api_key_here",
        model="glm-4-flash",
        max_tokens=512,
        temperature=0.7
    )
    
    print("=== GLM-4-Flash APIå®¢æˆ·ç«¯æ¼”ç¤º ===")
    
    async with GLMClient(config) as client:
        # ç®€å•èŠå¤©
        response = await client.simple_chat(
            user_message="ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±",
            system_prompt="ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIåŠ©æ‰‹ï¼Œåå«å°æ™ºã€‚"
        )
        
        if response.success:
            print(f"AIå›å¤: {response.content}")
            print(f"å“åº”æ—¶é—´: {response.response_time:.2f}ç§’")
            if response.usage:
                print(f"Tokenä½¿ç”¨: {response.usage}")
        else:
            print(f"è¯·æ±‚å¤±è´¥: {response.error_message}")
        
        # å¸¦ä¸Šä¸‹æ–‡çš„å¯¹è¯
        history = [
            ChatMessage(MessageRole.USER, "æˆ‘å«å¼ ä¸‰"),
            ChatMessage(MessageRole.ASSISTANT, "ä½ å¥½å¼ ä¸‰ï¼Œå¾ˆé«˜å…´è®¤è¯†ä½ ï¼")
        ]
        
        response = await client.chat_with_context(
            user_message="ä½ è¿˜è®°å¾—æˆ‘çš„åå­—å—ï¼Ÿ",
            conversation_history=history,
            system_prompt="ä½ æ˜¯ä¸€ä¸ªæœ‰è®°å¿†çš„AIåŠ©æ‰‹ã€‚"
        )
        
        if response.success:
            print(f"AIå›å¤ï¼ˆå¸¦ä¸Šä¸‹æ–‡ï¼‰: {response.content}")
        
        # æŸ¥çœ‹ç»Ÿè®¡ä¿¡æ¯
        stats = client.get_stats()
        print(f"APIè°ƒç”¨ç»Ÿè®¡: {stats}")

if __name__ == "__main__":
    asyncio.run(demo_glm_client())
```

## ğŸ¯ å®è·µç»ƒä¹ 

### ç»ƒä¹ 1ï¼šAPIé…ç½®ç®¡ç†
```python
class APIConfigManager:
    """
    APIé…ç½®ç®¡ç†ç»ƒä¹ 
    
    è¦æ±‚ï¼š
    1. æ”¯æŒå¤šä¸ªAPIæä¾›å•†é…ç½®
    2. å®ç°é…ç½®çƒ­æ›´æ–°
    3. æ·»åŠ é…ç½®éªŒè¯æœºåˆ¶
    4. æ”¯æŒç¯å¢ƒå˜é‡è¦†ç›–
    """
    
    def load_config_from_file(self, config_path: str) -> GLMConfig:
        """ä»æ–‡ä»¶åŠ è½½é…ç½®"""
        # TODO: å®ç°é…ç½®æ–‡ä»¶åŠ è½½
        pass
    
    def validate_config(self, config: GLMConfig) -> bool:
        """éªŒè¯é…ç½®æœ‰æ•ˆæ€§"""
        # TODO: å®ç°é…ç½®éªŒè¯
        pass
```

### ç»ƒä¹ 2ï¼šAPIå“åº”ç¼“å­˜
```python
class APIResponseCache:
    """
    APIå“åº”ç¼“å­˜ç»ƒä¹ 
    
    è¦æ±‚ï¼š
    1. å®ç°åŸºäºå†…å®¹å“ˆå¸Œçš„ç¼“å­˜
    2. æ”¯æŒTTLè¿‡æœŸæœºåˆ¶
    3. æ·»åŠ ç¼“å­˜å‘½ä¸­ç‡ç»Ÿè®¡
    4. å®ç°ç¼“å­˜æ¸…ç†ç­–ç•¥
    """
    
    def get_cached_response(self, request_hash: str) -> Optional[APIResponse]:
        """è·å–ç¼“å­˜å“åº”"""
        # TODO: å®ç°ç¼“å­˜æŸ¥è¯¢
        pass
    
    def cache_response(self, request_hash: str, response: APIResponse):
        """ç¼“å­˜å“åº”"""
        # TODO: å®ç°å“åº”ç¼“å­˜
        pass
```

## âœ… å­¦ä¹ æ£€æŸ¥

å®Œæˆæœ¬ç« å­¦ä¹ åï¼Œè¯·ç¡®è®¤æ‚¨èƒ½å¤Ÿï¼š

- [ ] ç†è§£AI APIé›†æˆçš„åŸºæœ¬æ¶æ„
- [ ] é…ç½®å’Œä½¿ç”¨GLM-4-Flash API
- [ ] å®ç°å¼‚æ­¥APIè°ƒç”¨å’Œé”™è¯¯å¤„ç†
- [ ] è®¾è®¡é™æµå’Œé‡è¯•æœºåˆ¶
- [ ] ç®¡ç†APIè°ƒç”¨ç»Ÿè®¡å’Œç›‘æ§
- [ ] å®Œæˆå®è·µç»ƒä¹ 

## ğŸ“š ä¸‹ä¸€æ­¥

APIé›†æˆåŸºç¡€æŒæ¡åï¼Œè¯·ç»§ç»­å­¦ä¹ ï¼š
- [GLM-4-Flashç‰¹æ€§](glm-4-flash-features.md)
- [ä¸Šä¸‹æ–‡ç®¡ç†](context-management.md)
- [å¼‚æ­¥å¤„ç†](async-processing.md)

---

**ç°åœ¨æ‚¨å·²ç»æŒæ¡äº†AI APIé›†æˆçš„æ ¸å¿ƒæŠ€æœ¯ï¼** ğŸ¤–
