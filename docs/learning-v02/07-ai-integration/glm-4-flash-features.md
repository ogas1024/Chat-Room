# GLM-4-Flashç‰¹æ€§è¯¦è§£

## ğŸ¯ å­¦ä¹ ç›®æ ‡

é€šè¿‡æœ¬ç« å­¦ä¹ ï¼Œæ‚¨å°†èƒ½å¤Ÿï¼š
- æ·±å…¥äº†è§£GLM-4-Flashæ¨¡å‹çš„ç‰¹æ€§å’Œèƒ½åŠ›
- æŒæ¡ä¸åŒåœºæ™¯ä¸‹çš„æç¤ºè¯å·¥ç¨‹æŠ€å·§
- å­¦ä¼šä¼˜åŒ–AIäº¤äº’ä½“éªŒå’Œå“åº”è´¨é‡
- åœ¨Chat-Roomé¡¹ç›®ä¸­å……åˆ†å‘æŒ¥AIåŠ©æ‰‹çš„æ½œåŠ›

## âš¡ GLM-4-Flashæ¨¡å‹ç‰¹æ€§

### æ¨¡å‹èƒ½åŠ›æ¦‚è§ˆ

```mermaid
graph TB
    subgraph "GLM-4-Flashæ ¸å¿ƒèƒ½åŠ›"
        A[è¯­è¨€ç†è§£<br/>Language Understanding] --> A1[å¤šè¯­è¨€æ”¯æŒ<br/>Multi-language]
        A --> A2[ä¸Šä¸‹æ–‡ç†è§£<br/>Context Awareness]
        A --> A3[æ„å›¾è¯†åˆ«<br/>Intent Recognition]
        
        B[å†…å®¹ç”Ÿæˆ<br/>Content Generation] --> B1[åˆ›æ„å†™ä½œ<br/>Creative Writing]
        B --> B2[ä»£ç ç”Ÿæˆ<br/>Code Generation]
        B --> B3[æ–‡æ¡£æ€»ç»“<br/>Summarization]
        
        C[å¯¹è¯äº¤äº’<br/>Conversational AI] --> C1[å¤šè½®å¯¹è¯<br/>Multi-turn Chat]
        C --> C2[è§’è‰²æ‰®æ¼”<br/>Role Playing]
        C --> C3[æƒ…æ„Ÿç†è§£<br/>Emotion Recognition]
        
        D[ä¸“ä¸šä»»åŠ¡<br/>Professional Tasks] --> D1[ç¿»è¯‘æœåŠ¡<br/>Translation]
        D --> D2[é—®ç­”ç³»ç»Ÿ<br/>Q&A System]
        D --> D3[å†…å®¹åˆ†æ<br/>Content Analysis]
    end
    
    subgraph "æŠ€æœ¯ç‰¹ç‚¹"
        E[é«˜é€Ÿå“åº”<br/>Fast Response]
        F[ä½å»¶è¿Ÿ<br/>Low Latency]
        G[é«˜å¹¶å‘<br/>High Concurrency]
        H[æˆæœ¬æ•ˆç›Š<br/>Cost Effective]
    end
    
    style A fill:#e8f5e8
    style B fill:#fff3cd
    style C fill:#f8d7da
    style D fill:#d1ecf1
```

### Chat-Room AIåŠ©æ‰‹åŠŸèƒ½è®¾è®¡

```mermaid
graph LR
    subgraph "AIåŠ©æ‰‹åŠŸèƒ½æ¨¡å—"
        A[æ™ºèƒ½é—®ç­”<br/>Smart Q&A] --> A1[æŠ€æœ¯å’¨è¯¢<br/>Tech Support]
        A --> A2[ç”Ÿæ´»åŠ©æ‰‹<br/>Life Assistant]
        A --> A3[å­¦ä¹ è¾…å¯¼<br/>Learning Help]
        
        B[å†…å®¹å¤„ç†<br/>Content Processing] --> B1[æ¶ˆæ¯æ€»ç»“<br/>Message Summary]
        B --> B2[å…³é”®è¯æå–<br/>Keyword Extract]
        B --> B3[æƒ…æ„Ÿåˆ†æ<br/>Sentiment Analysis]
        
        C[åˆ›æ„åä½œ<br/>Creative Collaboration] --> C1[å¤´è„‘é£æš´<br/>Brainstorming]
        C --> C2[æ–‡æ¡ˆåˆ›ä½œ<br/>Copywriting]
        C --> C3[ä»£ç åŠ©æ‰‹<br/>Code Assistant]
        
        D[ç¾¤ç»„ç®¡ç†<br/>Group Management] --> D1[è¯é¢˜å¼•å¯¼<br/>Topic Guide]
        D --> D2[æ°›å›´è°ƒèŠ‚<br/>Mood Regulation]
        D --> D3[æ´»åŠ¨å»ºè®®<br/>Activity Suggestion]
    end
    
    style A fill:#e8f5e8
    style B fill:#fff3cd
    style C fill:#f8d7da
    style D fill:#d1ecf1
```

## ğŸ¨ æç¤ºè¯å·¥ç¨‹

### Chat-Roomä¸“ç”¨æç¤ºè¯æ¨¡æ¿

```python
# server/ai/prompt_templates.py - æç¤ºè¯æ¨¡æ¿
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from enum import Enum
import json

class PromptType(Enum):
    """æç¤ºè¯ç±»å‹"""
    CHAT_ASSISTANT = "chat_assistant"
    CONTENT_SUMMARY = "content_summary"
    TRANSLATION = "translation"
    CODE_HELPER = "code_helper"
    CREATIVE_WRITING = "creative_writing"
    SENTIMENT_ANALYSIS = "sentiment_analysis"

@dataclass
class PromptTemplate:
    """æç¤ºè¯æ¨¡æ¿"""
    name: str
    type: PromptType
    system_prompt: str
    user_template: str
    parameters: Dict[str, Any]
    examples: List[Dict[str, str]]
    
    def format(self, **kwargs) -> str:
        """æ ¼å¼åŒ–ç”¨æˆ·æç¤ºè¯"""
        return self.user_template.format(**kwargs)

class ChatRoomPromptManager:
    """
    Chat-Roomæç¤ºè¯ç®¡ç†å™¨
    
    åŠŸèƒ½ï¼š
    1. ç®¡ç†å„ç§åœºæ™¯çš„æç¤ºè¯æ¨¡æ¿
    2. åŠ¨æ€ç”Ÿæˆä¸Šä¸‹æ–‡ç›¸å…³çš„æç¤ºè¯
    3. ä¼˜åŒ–AIå“åº”è´¨é‡å’Œä¸€è‡´æ€§
    4. æ”¯æŒå¤šè¯­è¨€å’Œä¸ªæ€§åŒ–å®šåˆ¶
    """
    
    def __init__(self):
        self.templates: Dict[str, PromptTemplate] = {}
        self._init_default_templates()
    
    def _init_default_templates(self):
        """åˆå§‹åŒ–é»˜è®¤æç¤ºè¯æ¨¡æ¿"""
        
        # èŠå¤©åŠ©æ‰‹æ¨¡æ¿
        chat_assistant = PromptTemplate(
            name="chat_assistant",
            type=PromptType.CHAT_ASSISTANT,
            system_prompt="""ä½ æ˜¯Chat-RoomèŠå¤©å®¤çš„AIåŠ©æ‰‹ï¼Œåå«"å°æ™º"ã€‚ä½ çš„ç‰¹ç‚¹ï¼š

1. å‹å¥½ã€è€å¿ƒã€ä¹äºåŠ©äºº
2. å…·æœ‰ä¸°å¯Œçš„çŸ¥è¯†å’Œç»éªŒ
3. èƒ½å¤Ÿç†è§£ä¸Šä¸‹æ–‡å’Œç”¨æˆ·æƒ…æ„Ÿ
4. å›å¤ç®€æ´æ˜äº†ï¼Œé¿å…è¿‡é•¿çš„å›ç­”
5. é€‚å½“ä½¿ç”¨è¡¨æƒ…ç¬¦å·å¢åŠ äº²å’ŒåŠ›
6. å°Šé‡ç”¨æˆ·éšç§ï¼Œä¸è®°å½•æ•æ„Ÿä¿¡æ¯

å½“å‰èŠå¤©ç¯å¢ƒï¼š
- ç¾¤ç»„åç§°ï¼š{group_name}
- åœ¨çº¿äººæ•°ï¼š{online_count}
- å½“å‰æ—¶é—´ï¼š{current_time}

è¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜å’ŒèŠå¤©ä¸Šä¸‹æ–‡ï¼Œæä¾›æœ‰å¸®åŠ©çš„å›å¤ã€‚""",
            user_template="ç”¨æˆ·é—®é¢˜ï¼š{user_message}\n\nèŠå¤©ä¸Šä¸‹æ–‡ï¼š\n{chat_context}",
            parameters={
                "max_tokens": 512,
                "temperature": 0.7,
                "top_p": 0.9
            },
            examples=[
                {
                    "user": "ä½ å¥½ï¼Œå°æ™ºï¼",
                    "assistant": "ä½ å¥½ï¼ğŸ˜Š å¾ˆé«˜å…´åœ¨Chat-Roomé‡åˆ°ä½ ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ"
                },
                {
                    "user": "è¿™ä¸ªç¾¤é‡Œæœ‰ä»€ä¹ˆæœ‰è¶£çš„è¯é¢˜å—ï¼Ÿ",
                    "assistant": "è®©æˆ‘çœ‹çœ‹æœ€è¿‘çš„èŠå¤©è®°å½•...å¤§å®¶åœ¨è®¨è®ºæŠ€æœ¯ã€ç”Ÿæ´»å’Œå­¦ä¹ ç­‰è¯é¢˜å‘¢ï¼ä½ å¯¹å“ªä¸ªæ–¹é¢æ¯”è¾ƒæ„Ÿå…´è¶£ï¼Ÿæˆ‘å¯ä»¥å¸®ä½ æ‰¾åˆ°ç›¸å…³çš„è®¨è®º ğŸ¤”"
                }
            ]
        )
        
        # å†…å®¹æ€»ç»“æ¨¡æ¿
        content_summary = PromptTemplate(
            name="content_summary",
            type=PromptType.CONTENT_SUMMARY,
            system_prompt="""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å†…å®¹æ€»ç»“åŠ©æ‰‹ã€‚è¯·å¯¹èŠå¤©è®°å½•è¿›è¡Œç®€æ´ã€å‡†ç¡®çš„æ€»ç»“ã€‚

æ€»ç»“è¦æ±‚ï¼š
1. æå–å…³é”®ä¿¡æ¯å’Œä¸»è¦è¯é¢˜
2. ä¿æŒå®¢è§‚ä¸­æ€§çš„è¯­è°ƒ
3. çªå‡ºé‡è¦å†³å®šå’Œç»“è®º
4. æ§åˆ¶åœ¨200å­—ä»¥å†…
5. ä½¿ç”¨æ¸…æ™°çš„ç»“æ„åŒ–æ ¼å¼""",
            user_template="è¯·æ€»ç»“ä»¥ä¸‹èŠå¤©å†…å®¹ï¼š\n\n{chat_content}\n\næ€»ç»“è¦ç‚¹ï¼š{summary_points}",
            parameters={
                "max_tokens": 300,
                "temperature": 0.3,
                "top_p": 0.8
            },
            examples=[
                {
                    "user": "æ€»ç»“ä»Šå¤©çš„æŠ€æœ¯è®¨è®º",
                    "assistant": "ğŸ“‹ ä»Šæ—¥æŠ€æœ¯è®¨è®ºæ€»ç»“ï¼š\n\nğŸ”¹ ä¸»è¦è¯é¢˜ï¼šPythonå¼‚æ­¥ç¼–ç¨‹\nğŸ”¹ è®¨è®ºè¦ç‚¹ï¼šasyncioåº“ä½¿ç”¨ã€æ€§èƒ½ä¼˜åŒ–\nğŸ”¹ è¾¾æˆå…±è¯†ï¼šå¼‚æ­¥ç¼–ç¨‹é€‚åˆI/Oå¯†é›†å‹ä»»åŠ¡\nğŸ”¹ åç»­è®¡åˆ’ï¼šä¸‹å‘¨åˆ†äº«å®æˆ˜æ¡ˆä¾‹"
                }
            ]
        )
        
        # ç¿»è¯‘åŠ©æ‰‹æ¨¡æ¿
        translation = PromptTemplate(
            name="translation",
            type=PromptType.TRANSLATION,
            system_prompt="""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¿»è¯‘åŠ©æ‰‹ã€‚è¯·æä¾›å‡†ç¡®ã€è‡ªç„¶çš„ç¿»è¯‘æœåŠ¡ã€‚

ç¿»è¯‘åŸåˆ™ï¼š
1. ä¿æŒåŸæ–‡çš„è¯­æ°”å’Œé£æ ¼
2. è€ƒè™‘æ–‡åŒ–èƒŒæ™¯å’Œè¯­å¢ƒ
3. ä½¿ç”¨åœ°é“çš„è¡¨è¾¾æ–¹å¼
4. å¯¹äºä¸“ä¸šæœ¯è¯­ï¼Œæä¾›å‡†ç¡®ç¿»è¯‘
5. å¦‚æœ‰æ­§ä¹‰ï¼Œæä¾›å¤šç§ç¿»è¯‘é€‰é¡¹""",
            user_template="è¯·å°†ä»¥ä¸‹å†…å®¹ä»{source_lang}ç¿»è¯‘ä¸º{target_lang}ï¼š\n\n{text}",
            parameters={
                "max_tokens": 1024,
                "temperature": 0.2,
                "top_p": 0.8
            },
            examples=[
                {
                    "user": "ç¿»è¯‘ï¼šHello, how are you?",
                    "assistant": "ä½ å¥½ï¼Œä½ å¥½å—ï¼Ÿ\n\nğŸ’¡ æ›´è‡ªç„¶çš„è¡¨è¾¾ï¼š\n- ä½ å¥½ï¼Œæœ€è¿‘æ€ä¹ˆæ ·ï¼Ÿ\n- å—¨ï¼Œä½ è¿˜å¥½å—ï¼Ÿ"
                }
            ]
        )
        
        # ä»£ç åŠ©æ‰‹æ¨¡æ¿
        code_helper = PromptTemplate(
            name="code_helper",
            type=PromptType.CODE_HELPER,
            system_prompt="""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¼–ç¨‹åŠ©æ‰‹ã€‚è¯·æä¾›å‡†ç¡®ã€å®ç”¨çš„ç¼–ç¨‹å¸®åŠ©ã€‚

æœåŠ¡å†…å®¹ï¼š
1. ä»£ç è§£é‡Šå’Œåˆ†æ
2. é”™è¯¯è¯Šæ–­å’Œä¿®å¤å»ºè®®
3. ä»£ç ä¼˜åŒ–å’Œé‡æ„
4. æœ€ä½³å®è·µæ¨è
5. ç®—æ³•å’Œæ•°æ®ç»“æ„æŒ‡å¯¼

å›å¤æ ¼å¼ï¼š
- ç®€æ´çš„è§£é‡Š
- å®Œæ•´çš„ä»£ç ç¤ºä¾‹
- å…³é”®ç‚¹è¯´æ˜
- ç›¸å…³å»ºè®®""",
            user_template="ç¼–ç¨‹é—®é¢˜ï¼š{question}\n\nä»£ç ä¸Šä¸‹æ–‡ï¼š\n```{language}\n{code}\n```\n\nå…·ä½“éœ€æ±‚ï¼š{requirements}",
            parameters={
                "max_tokens": 1024,
                "temperature": 0.1,
                "top_p": 0.9
            },
            examples=[
                {
                    "user": "å¦‚ä½•ä¼˜åŒ–è¿™ä¸ªPythonå¾ªç¯ï¼Ÿ",
                    "assistant": "ğŸ”§ ä»£ç ä¼˜åŒ–å»ºè®®ï¼š\n\n```python\n# åŸä»£ç ä¼˜åŒ–ç‰ˆæœ¬\nresult = [process(item) for item in items if condition(item)]\n```\n\nğŸ’¡ ä¼˜åŒ–è¦ç‚¹ï¼š\n- ä½¿ç”¨åˆ—è¡¨æ¨å¯¼å¼æé«˜æ€§èƒ½\n- å‡å°‘å‡½æ•°è°ƒç”¨å¼€é”€\n- æé«˜ä»£ç å¯è¯»æ€§"
                }
            ]
        )
        
        # æƒ…æ„Ÿåˆ†ææ¨¡æ¿
        sentiment_analysis = PromptTemplate(
            name="sentiment_analysis",
            type=PromptType.SENTIMENT_ANALYSIS,
            system_prompt="""ä½ æ˜¯ä¸€ä¸ªæƒ…æ„Ÿåˆ†æä¸“å®¶ã€‚è¯·åˆ†ææ–‡æœ¬çš„æƒ…æ„Ÿå€¾å‘å’Œæƒ…ç»ªçŠ¶æ€ã€‚

åˆ†æç»´åº¦ï¼š
1. æƒ…æ„Ÿææ€§ï¼šæ­£é¢ã€è´Ÿé¢ã€ä¸­æ€§
2. æƒ…æ„Ÿå¼ºåº¦ï¼šå¼ºçƒˆã€ä¸­ç­‰ã€è½»å¾®
3. å…·ä½“æƒ…ç»ªï¼šå¼€å¿ƒã€æ„¤æ€’ã€æ‚²ä¼¤ã€ç„¦è™‘ç­‰
4. æƒ…æ„Ÿå˜åŒ–ï¼šæƒ…ç»ªæ³¢åŠ¨è¶‹åŠ¿

è¾“å‡ºæ ¼å¼ï¼š
- ç®€æ´çš„åˆ†æç»“æœ
- å…³é”®æƒ…æ„Ÿè¯æ±‡
- å»ºè®®çš„å›åº”æ–¹å¼""",
            user_template="è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬çš„æƒ…æ„Ÿï¼š\n\n{text}\n\nåˆ†æé‡ç‚¹ï¼š{focus_areas}",
            parameters={
                "max_tokens": 256,
                "temperature": 0.3,
                "top_p": 0.8
            },
            examples=[
                {
                    "user": "åˆ†æï¼šä»Šå¤©å·¥ä½œå¤ªç´¯äº†ï¼Œä»€ä¹ˆéƒ½ä¸æƒ³åš",
                    "assistant": "ğŸ˜” æƒ…æ„Ÿåˆ†æï¼š\n\nğŸ”¹ æƒ…æ„Ÿææ€§ï¼šè´Ÿé¢\nğŸ”¹ æƒ…æ„Ÿå¼ºåº¦ï¼šä¸­ç­‰\nğŸ”¹ ä¸»è¦æƒ…ç»ªï¼šç–²æƒ«ã€æ¶ˆæ\nğŸ”¹ å»ºè®®å›åº”ï¼šè¡¨ç¤ºç†è§£å’Œå…³å¿ƒï¼Œæä¾›æ”¾æ¾å»ºè®®"
                }
            ]
        )
        
        # æ³¨å†Œæ¨¡æ¿
        self.templates = {
            "chat_assistant": chat_assistant,
            "content_summary": content_summary,
            "translation": translation,
            "code_helper": code_helper,
            "sentiment_analysis": sentiment_analysis
        }
    
    def get_template(self, template_name: str) -> Optional[PromptTemplate]:
        """è·å–æç¤ºè¯æ¨¡æ¿"""
        return self.templates.get(template_name)
    
    def format_chat_prompt(self, user_message: str, group_name: str = "é»˜è®¤ç¾¤ç»„",
                          online_count: int = 1, chat_context: str = "") -> tuple[str, str]:
        """æ ¼å¼åŒ–èŠå¤©æç¤ºè¯"""
        template = self.get_template("chat_assistant")
        if not template:
            return "", user_message
        
        import datetime
        current_time = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        system_prompt = template.system_prompt.format(
            group_name=group_name,
            online_count=online_count,
            current_time=current_time
        )
        
        user_prompt = template.format(
            user_message=user_message,
            chat_context=chat_context
        )
        
        return system_prompt, user_prompt
    
    def format_summary_prompt(self, chat_content: str, 
                            summary_points: str = "ä¸»è¦è¯é¢˜å’Œç»“è®º") -> tuple[str, str]:
        """æ ¼å¼åŒ–æ€»ç»“æç¤ºè¯"""
        template = self.get_template("content_summary")
        if not template:
            return "", chat_content
        
        system_prompt = template.system_prompt
        user_prompt = template.format(
            chat_content=chat_content,
            summary_points=summary_points
        )
        
        return system_prompt, user_prompt
    
    def format_translation_prompt(self, text: str, source_lang: str = "è‹±æ–‡",
                                target_lang: str = "ä¸­æ–‡") -> tuple[str, str]:
        """æ ¼å¼åŒ–ç¿»è¯‘æç¤ºè¯"""
        template = self.get_template("translation")
        if not template:
            return "", text
        
        system_prompt = template.system_prompt
        user_prompt = template.format(
            text=text,
            source_lang=source_lang,
            target_lang=target_lang
        )
        
        return system_prompt, user_prompt
    
    def format_code_helper_prompt(self, question: str, code: str = "",
                                language: str = "python", 
                                requirements: str = "è¯·æä¾›è§£å†³æ–¹æ¡ˆ") -> tuple[str, str]:
        """æ ¼å¼åŒ–ä»£ç åŠ©æ‰‹æç¤ºè¯"""
        template = self.get_template("code_helper")
        if not template:
            return "", question
        
        system_prompt = template.system_prompt
        user_prompt = template.format(
            question=question,
            code=code,
            language=language,
            requirements=requirements
        )
        
        return system_prompt, user_prompt
    
    def add_custom_template(self, template: PromptTemplate):
        """æ·»åŠ è‡ªå®šä¹‰æ¨¡æ¿"""
        self.templates[template.name] = template
    
    def list_templates(self) -> List[str]:
        """åˆ—å‡ºæ‰€æœ‰æ¨¡æ¿åç§°"""
        return list(self.templates.keys())

# æ™ºèƒ½æç¤ºè¯ä¼˜åŒ–å™¨
class PromptOptimizer:
    """æç¤ºè¯ä¼˜åŒ–å™¨"""
    
    def __init__(self):
        self.optimization_rules = {
            "clarity": "ä½¿ç”¨æ¸…æ™°ã€å…·ä½“çš„è¯­è¨€",
            "context": "æä¾›å……åˆ†çš„ä¸Šä¸‹æ–‡ä¿¡æ¯",
            "structure": "ä½¿ç”¨ç»“æ„åŒ–çš„æ ¼å¼",
            "examples": "åŒ…å«ç›¸å…³çš„ç¤ºä¾‹",
            "constraints": "æ˜ç¡®æŒ‡å®šè¾“å‡ºçº¦æŸ"
        }
    
    def optimize_prompt(self, original_prompt: str, 
                       optimization_goals: List[str]) -> str:
        """ä¼˜åŒ–æç¤ºè¯"""
        optimized = original_prompt
        
        for goal in optimization_goals:
            if goal == "clarity":
                optimized = self._improve_clarity(optimized)
            elif goal == "structure":
                optimized = self._add_structure(optimized)
            elif goal == "examples":
                optimized = self._add_examples(optimized)
        
        return optimized
    
    def _improve_clarity(self, prompt: str) -> str:
        """æé«˜æ¸…æ™°åº¦"""
        # æ·»åŠ æ˜ç¡®çš„æŒ‡ä»¤
        if not prompt.startswith("è¯·"):
            prompt = "è¯·" + prompt
        
        # æ·»åŠ è¾“å‡ºæ ¼å¼è¯´æ˜
        if "æ ¼å¼" not in prompt:
            prompt += "\n\nè¯·ä»¥æ¸…æ™°ã€ç®€æ´çš„æ ¼å¼å›å¤ã€‚"
        
        return prompt
    
    def _add_structure(self, prompt: str) -> str:
        """æ·»åŠ ç»“æ„åŒ–æ ¼å¼"""
        if "æ­¥éª¤" not in prompt and "è¦ç‚¹" not in prompt:
            prompt += "\n\nè¯·æŒ‰ä»¥ä¸‹ç»“æ„å›å¤ï¼š\n1. ä¸»è¦è§‚ç‚¹\n2. è¯¦ç»†è¯´æ˜\n3. æ€»ç»“å»ºè®®"
        
        return prompt
    
    def _add_examples(self, prompt: str) -> str:
        """æ·»åŠ ç¤ºä¾‹"""
        if "ä¾‹å¦‚" not in prompt and "ç¤ºä¾‹" not in prompt:
            prompt += "\n\nè¯·æä¾›å…·ä½“çš„ä¾‹å­æ¥è¯´æ˜ä½ çš„è§‚ç‚¹ã€‚"
        
        return prompt

# ä½¿ç”¨ç¤ºä¾‹
def demo_prompt_management():
    """æç¤ºè¯ç®¡ç†æ¼”ç¤º"""
    manager = ChatRoomPromptManager()
    
    print("=== Chat-Roomæç¤ºè¯ç®¡ç†æ¼”ç¤º ===")
    
    # èŠå¤©åŠ©æ‰‹æç¤ºè¯
    system_prompt, user_prompt = manager.format_chat_prompt(
        user_message="ä½ èƒ½å¸®æˆ‘è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ å—ï¼Ÿ",
        group_name="AIå­¦ä¹ ç¾¤",
        online_count=15,
        chat_context="æœ€è¿‘å¤§å®¶åœ¨è®¨è®ºäººå·¥æ™ºèƒ½çš„å‘å±•è¶‹åŠ¿"
    )
    
    print("èŠå¤©åŠ©æ‰‹æç¤ºè¯ï¼š")
    print(f"ç³»ç»Ÿæç¤ºè¯ï¼š{system_prompt[:200]}...")
    print(f"ç”¨æˆ·æç¤ºè¯ï¼š{user_prompt}")
    
    # ä»£ç åŠ©æ‰‹æç¤ºè¯
    system_prompt, user_prompt = manager.format_code_helper_prompt(
        question="å¦‚ä½•ä¼˜åŒ–è¿™ä¸ªå‡½æ•°çš„æ€§èƒ½ï¼Ÿ",
        code="def slow_function(data):\n    result = []\n    for item in data:\n        if item > 0:\n            result.append(item * 2)\n    return result",
        language="python",
        requirements="æä¾›æ›´é«˜æ•ˆçš„å®ç°æ–¹å¼"
    )
    
    print("\nä»£ç åŠ©æ‰‹æç¤ºè¯ï¼š")
    print(f"ç”¨æˆ·æç¤ºè¯ï¼š{user_prompt}")
    
    # åˆ—å‡ºæ‰€æœ‰æ¨¡æ¿
    templates = manager.list_templates()
    print(f"\nå¯ç”¨æ¨¡æ¿ï¼š{templates}")

if __name__ == "__main__":
    demo_prompt_management()
```

## ğŸ¯ å®è·µç»ƒä¹ 

### ç»ƒä¹ 1ï¼šä¸ªæ€§åŒ–AIåŠ©æ‰‹
```python
class PersonalizedAIAssistant:
    """
    ä¸ªæ€§åŒ–AIåŠ©æ‰‹ç»ƒä¹ 
    
    è¦æ±‚ï¼š
    1. æ ¹æ®ç”¨æˆ·åå¥½è°ƒæ•´å›å¤é£æ ¼
    2. å­¦ä¹ ç”¨æˆ·çš„å…´è¶£å’Œä¹ æƒ¯
    3. æä¾›ä¸ªæ€§åŒ–çš„å»ºè®®å’ŒæœåŠ¡
    4. æ”¯æŒå¤šç§AIäººæ ¼è®¾å®š
    """
    
    def adapt_to_user_style(self, user_id: int, conversation_history: List[str]):
        """é€‚åº”ç”¨æˆ·é£æ ¼"""
        # TODO: å®ç°ç”¨æˆ·é£æ ¼å­¦ä¹ 
        pass
```

### ç»ƒä¹ 2ï¼šå¤šæ¨¡æ€AIé›†æˆ
```python
class MultiModalAI:
    """
    å¤šæ¨¡æ€AIé›†æˆç»ƒä¹ 
    
    è¦æ±‚ï¼š
    1. æ”¯æŒå›¾åƒç†è§£å’Œæè¿°
    2. é›†æˆè¯­éŸ³è¯†åˆ«å’Œåˆæˆ
    3. å¤„ç†æ–‡æ¡£å’Œè¡¨æ ¼æ•°æ®
    4. å®ç°è·¨æ¨¡æ€çš„æ™ºèƒ½äº¤äº’
    """
    
    def process_image_message(self, image_data: bytes, user_query: str):
        """å¤„ç†å›¾åƒæ¶ˆæ¯"""
        # TODO: å®ç°å›¾åƒç†è§£
        pass
```

## âœ… å­¦ä¹ æ£€æŸ¥

å®Œæˆæœ¬ç« å­¦ä¹ åï¼Œè¯·ç¡®è®¤æ‚¨èƒ½å¤Ÿï¼š

- [ ] ç†è§£GLM-4-Flashæ¨¡å‹çš„æ ¸å¿ƒç‰¹æ€§
- [ ] è®¾è®¡æœ‰æ•ˆçš„æç¤ºè¯æ¨¡æ¿
- [ ] ä¼˜åŒ–AIäº¤äº’ä½“éªŒ
- [ ] å®ç°å¤šåœºæ™¯çš„AIåŠŸèƒ½
- [ ] ç®¡ç†å’Œä¼˜åŒ–æç¤ºè¯è´¨é‡
- [ ] å®Œæˆå®è·µç»ƒä¹ 

## ğŸ“š ä¸‹ä¸€æ­¥

GLM-4-Flashç‰¹æ€§æŒæ¡åï¼Œè¯·ç»§ç»­å­¦ä¹ ï¼š
- [ä¸Šä¸‹æ–‡ç®¡ç†](context-management.md)
- [å¼‚æ­¥å¤„ç†](async-processing.md)

---

**ç°åœ¨æ‚¨å·²ç»æŒæ¡äº†GLM-4-Flashçš„å¼ºå¤§ç‰¹æ€§ï¼** âš¡
